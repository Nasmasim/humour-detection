{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "run_GloVe_biLSTM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nasmasim/humour-detection/blob/main/run_GloVe_biLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkOV493FZNPS"
      },
      "source": [
        "**In this Notebook, we present the baseline approach using GloVe word embeddings combined with a biLSTM to predict the funniness score of a news headline from the humicroedit dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "201Mzn8FZNPT"
      },
      "source": [
        "**1. Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwGuEkijZNPU"
      },
      "source": [
        "In Natural language Processing, humour detection is a challenging task. The [SemEval-2020 Task 7](https://arxiv.org/pdf/2008.00304.pdf)  aims to detect humour in English news headlines from micro-edits. The humicroedit dataset (Hossain et al., 2020) contains 9653 training, 2420 development and 3025 testing examples. In task 1 the goal is to predict the the funniness score of an edited headline in the ranges of 0 to 3, where 0 means not funny and 3 means very funny. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR5vA5lTZNPU"
      },
      "source": [
        "**2. Import and Downloads**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8qrPazCaS1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6949928-1566-4c62-ade7-b28865d348b2"
      },
      "source": [
        "# mount project to drive\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "# set\n",
        "py_file_location = \"/content/drive/MyDrive/humour-detection\"\n",
        "sys.path.append(os.path.abspath(py_file_location))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSd_eRaXZNPU",
        "outputId": "9978b072-ad67-4ab0-ecce-006751ac3a06"
      },
      "source": [
        "#@title Download and Install\n",
        "# Check GPU\n",
        "!nvidia-smi\n",
        "# Baseline data collection- for full download \n",
        "#!wget -nc http://nlp.stanford.edu/data/glove.6B.zip -O glove.6B.zip\n",
        "#!unzip -n glove.6B.zip\n",
        "# installing transformers\n",
        "!pip -q install nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May 24 08:26:16 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    57W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-HU6YFLZNPV"
      },
      "source": [
        "#@title Imports\n",
        "# Library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from preprocessing.preprocessor import (create_edited_sentences, dataset_question_full_processing,\n",
        "                                        create_custom_vocab, get_stop_words)\n",
        "from dataloader.data_loaders import (Task1Dataset, collate_fn_padd, get_input_bert,\n",
        "                                      get_dataloaders, get_dataloaders_no_random_split)\n",
        "from models.biLSTM import BiLSTM\n",
        "from trainer.biLSTM_trainer import biLSTM_train, biLSTM_eval\n",
        "from utils.plot import (plot_sentence_length_stopwords, plot_mean_grade_distribution,\n",
        "                        plot_number_characters, plot_number_words, plot_top_ngrams, \n",
        "                        plot_loss_vs_epochs)\n",
        "from utils.ngrams import get_top_ngram\n",
        "from utils.vocab import create_vocab, get_word2idx"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN_LvBmAZNPW"
      },
      "source": [
        "**3. Settings and Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XULrCUbAZNPW"
      },
      "source": [
        "#@title Torch Settings\n",
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Number of epochs\n",
        "epochs_bl = 10 \n",
        "epochs = 100\n",
        "\n",
        "# Proportion of training data for train compared to dev\n",
        "train_proportion = 0.8"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4YzSQEoZNPW"
      },
      "source": [
        "**4. Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhh7nNR-ZNPW"
      },
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv(py_file_location+'/data/task-1/train.csv')\n",
        "dev_df = pd.read_csv(py_file_location+'/data/task-1/dev.csv')\n",
        "test_df = pd.read_csv(py_file_location+'/data/task-1/test.csv')\n",
        "\n",
        "# Convert them to full edited sentences\n",
        "modified_train_df = create_edited_sentences(train_df)\n",
        "modified_valid_df = create_edited_sentences(dev_df)\n",
        "modified_test_df = create_edited_sentences(test_df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ADHXB67ZNPX"
      },
      "source": [
        "**5. Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8yYqsfMZNPX"
      },
      "source": [
        "***Stop Words***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKv6zVXvZNPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47fa9c4-cc8a-4ff7-e3a2-59696f4182a8"
      },
      "source": [
        "# nltk stopwords english list\n",
        "nltk.download('stopwords')\n",
        "nltk_stopwords = list(stopwords.words('english'))\n",
        "\n",
        "# import custom stopword list\n",
        "simple_stopwords, custom_stopwords = get_stop_words()\n",
        "\n",
        "all_stopwords = list(set(custom_stopwords + nltk_stopwords))\n",
        "\n",
        "stopwords_lists = [[],simple_stopwords, custom_stopwords,nltk_stopwords,\\\n",
        "                   all_stopwords]\n",
        "edited_modes = ['question_edited','full_edited']\n",
        "\n",
        "# OK Applying both question and full version to any dataframe and dropping useless values\n",
        "modified_train_df = dataset_question_full_processing(modified_train_df)\n",
        "modified_valid_df = dataset_question_full_processing(modified_valid_df)\n",
        "modified_test_df = dataset_question_full_processing(modified_test_df)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8KuzGkCZNPX"
      },
      "source": [
        "***Word2idx***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw8YpNxUZNPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b137ef-8f69-4caa-bd27-530bacd7442d"
      },
      "source": [
        "#@title Word2idx call on the original sentences\n",
        "## Approach 1 code, using functions defined above:\n",
        "\n",
        "# We set our training data and test data\n",
        "training_data = train_df['original']\n",
        "test_data = dev_df['original']\n",
        "\n",
        "# Creating word vectors\n",
        "training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
        "test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
        "# Creating joint vocab from test and train:\n",
        "joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data]))\n",
        "print(\"Vocab created.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU6NuMU9ZNPY"
      },
      "source": [
        "file_path =py_file_location+'/embeddings/glove.6B.100d.txt'\n",
        "wvecs, word2idx, idx2word= get_word2idx(file_path, joint_vocab)\n",
        "\n",
        "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\n",
        "\n",
        "# To avoid any sentences being empty (if no words match to our word embeddings)\n",
        "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSLMYoWSZNPY"
      },
      "source": [
        "**6. Train GloVE + biLSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlKKwQMfZNPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37734923-5aa4-4da2-d3bf-05bfb61d1e9f"
      },
      "source": [
        "#@title Train BiLSTM on original sentences\n",
        "INPUT_DIM = len(word2idx)\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "model = BiLSTM(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\n",
        "print(\"Model initialised.\")\n",
        "\n",
        "model.to(device)\n",
        "# We provide the model with our embeddings\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
        "\n",
        "feature = vectorized_seqs\n",
        "\n",
        "# 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
        "train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\n",
        "\n",
        "train_examples = round(len(train_and_dev)*train_proportion)\n",
        "dev_examples = len(train_and_dev) - train_examples\n",
        "\n",
        "train_dataset, dev_dataset = random_split(train_and_dev,\n",
        "                                           (train_examples,\n",
        "                                            dev_examples))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "\n",
        "print(\"Dataloaders created.\")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_fn = loss_fn.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialised.\n",
            "Dataloaders created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x692DrFDZNPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21eb23f1-5ff2-44c1-dc8e-01fc813d2c1f"
      },
      "source": [
        "biLSTM_train(train_loader, dev_loader, model, epochs_bl, device, optimizer, loss_fn)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 05 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
            "| Epoch: 06 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
            "| Epoch: 07 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
            "| Epoch: 08 | Train Loss: 0.24 | Train MSE: 0.24 | Train RMSE: 0.49 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n",
            "| Epoch: 09 | Train Loss: 0.23 | Train MSE: 0.23 | Train RMSE: 0.48 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.61 |\n",
            "| Epoch: 10 | Train Loss: 0.22 | Train MSE: 0.22 | Train RMSE: 0.47 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.62 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}