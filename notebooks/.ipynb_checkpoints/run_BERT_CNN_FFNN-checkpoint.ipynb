{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this Notebook, we present an approach using pretrained BERT word embeddings combined with CNN and FFNN to predict the funniness score of a news headline from the humicroedit dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural language Processing, humour detection is a challenging task. The [SemEval-2020 Task 7](https://arxiv.org/pdf/2008.00304.pdf)  aims to detect humour in English news headlines from micro-edits. The humicroedit dataset (Hossain et al., 2020) contains 9653 training, 2420 development and 3025 testing examples. In task 1 the goal is to predict the the funniness score of an edited headline in the ranges of 0 to 3, where 0 means not funny and 3 means very funny. \n",
    "\n",
    "The main idea is that we want to use a context-sensitive embedding as the first layer in building a deep learning regression model. Whereas static word embeddings like Word2Vec or GLoVE give the same meaning to a word in different contexts, a dynamic (contextualized) word embedding trained on a general language model task with a large corpus is able to represent polysemy, capture long- term dependencies in language and help the model learn sentiments. We care about contextualized embeddings because humor is often conveyed through puns, quirky expressions and parodies, which involve using word combinations in unusual ways.\n",
    "\n",
    "We chose BERT because it is both task-agnostic and deeply bidirectional. As opposed to a bidirectional RNN model, which is not truly bidirectional as states from the two directions do not interact with each other, BERT representations are jointly conditioned on both left and right context in all layers.\n",
    "\n",
    "For the experiment, we use the PyTorch implementation of the smallest base variant (’bert-base-uncased’) of the pre-trained BERT models by HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Import and Downloads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "#@title Download and Install\n",
    "# Check GPU\n",
    "!nvidia-smi\n",
    "# installing transformers\n",
    "!pip -q install transformers\n",
    "!pip -q install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "# Library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from preprocessing.preprocessor import (create_edited_sentences, dataset_question_full_processing,\n",
    "                                        create_custom_vocab, get_stop_words)\n",
    "from dataloader.data_loaders import (Task1Dataset, collate_fn_padd, get_input_bert,\n",
    "                                      get_dataloaders, get_dataloaders_no_random_split)\n",
    "from models.BERT_FFNN import FFNN\n",
    "from models.BERT_CNN import CNN\n",
    "from trainer.BERT_trainer import bert_eval, bert_train\n",
    "from utils.plot import (plot_sentence_length_stopwords, plot_mean_grade_distribution,\n",
    "                        plot_number_characters, plot_number_words, plot_top_ngrams, \n",
    "                        plot_loss_vs_epochs)\n",
    "from utils.ngrams import get_top_ngram\n",
    "from utils.vocab import create_vocab, get_word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Settings and Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Torch Settings\n",
    "# Setting random seed and device\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Number of epochs\n",
    "epochs_bl = 10 \n",
    "epochs = 100\n",
    "\n",
    "# Proportion of training data for train compared to dev\n",
    "train_proportion = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('data/task-1/train.csv')\n",
    "dev_df = pd.read_csv('data/task-1/dev.csv')\n",
    "test_df = pd.read_csv('data/task-1/test.csv')\n",
    "\n",
    "# Convert them to full edited sentences\n",
    "modified_train_df = create_edited_sentences(train_df)\n",
    "modified_valid_df = create_edited_sentences(dev_df)\n",
    "modified_test_df = create_edited_sentences(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are interested in a regression task that gives an absolute score for humor instead of comparing two headlines, we directly replaced the word in </> in the original headline with the word given for micro- edits. In the first preprocessing approach (function ```question_sentence_preprocessing``` in the notebook), we took the edited headlines, converted them to all lower cases, converted *’t* was converted to *not*, and removed a subset of punctuation (kept question mark) and trailing white spaces. The second preprocessing approach\n",
    "(```full_sentence_preprocessing``` in the note- book) is more straightforward, in which we converted all words into lower cases, removed all special char- acters and trailing white spaces. We used these two preprocessing approaches because the BERT model was trained with special characters, and we wanted to examine if different levels of text preprocessing would make a difference in the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Stop Words***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nasmadasser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk stopwords english list\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = list(stopwords.words('english'))\n",
    "\n",
    "# import custom stopword list\n",
    "simple_stopwords, custom_stopwords = get_stop_words()\n",
    "\n",
    "all_stopwords = list(set(custom_stopwords + nltk_stopwords))\n",
    "\n",
    "stopwords_lists = [[],simple_stopwords, custom_stopwords,nltk_stopwords,\\\n",
    "                   all_stopwords]\n",
    "edited_modes = ['question_edited','full_edited']\n",
    "\n",
    "# OK Applying both question and full version to any dataframe and dropping useless values\n",
    "modified_train_df = dataset_question_full_processing(modified_train_df)\n",
    "modified_valid_df = dataset_question_full_processing(modified_valid_df)\n",
    "modified_test_df = dataset_question_full_processing(modified_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BERT Embeddings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert download\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. BERT + Feed Forward Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***a. FFNN question-sentence preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised.\n",
      "Dataloaders created.\n"
     ]
    }
   ],
   "source": [
    "## FFNN question-sentence preprocessing\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "bert_ffnn = FFNN()\n",
    "print(\"Model initialised.\")\n",
    "bert_ffnn.to(device)\n",
    "\n",
    "# Get inputs for BERT and data loaders\n",
    "input_id_mask_train = get_input_bert(df=modified_train_df, tokenizer = tokenizer, col=\"question_edited\")\n",
    "\n",
    "input_id_mask_valid = get_input_bert(df= modified_valid_df, tokenizer=tokenizer, col =\"question_edited\") \n",
    "\n",
    "train_loader, dev_loader = get_dataloaders_no_random_split(input_data_train=input_id_mask_train, \n",
    "                                                            targets_train=modified_train_df['meanGrade'],\n",
    "                                                            input_data_valid=input_id_mask_valid,\n",
    "                                                            targets_valid=modified_valid_df['meanGrade'], \n",
    "                                                            batch_size = BATCH_SIZE) \n",
    "\n",
    "print(\"Dataloaders created.\")\n",
    "# calculate loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer_ff = torch.optim.Adam(bert_ffnn.parameters(), lr=1e-5)\n",
    "\n",
    "bert_model = bert_model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train BERT + FFNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_train_losses_ff, question_valid_losses_ff = bert_train(\n",
    "     optimizer=optimizer_ff, \n",
    "     train_iter=train_loader, \n",
    "     dev_iter=dev_loader, \n",
    "     model=bert_ffnn,\n",
    "     loss_fn=loss_fn,\n",
    "     device= device, \n",
    "     number_epoch = 10,     \n",
    "     model_name='task1_question_ffnn.pt', \n",
    "     patience=5, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***b. FFNN Full-sentence preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised.\n",
      "Dataloaders created.\n"
     ]
    }
   ],
   "source": [
    "## FFNN Full-sentence preprocessing\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "bert_ffnn_full = FFNN()\n",
    "print(\"Model initialised.\")\n",
    "\n",
    "bert_ffnn_full.to(device)\n",
    "\n",
    "# Get inputs for BERT and data loaders\n",
    "input_id_mask_train = get_input_bert(df= modified_train_df, tokenizer= tokenizer, col= \"full_edited\")\n",
    "input_id_mask_valid = get_input_bert(df= modified_valid_df, tokenizer= tokenizer, col= \"full_edited\") \n",
    "train_loader, dev_loader = get_dataloaders_no_random_split(\n",
    "    input_data_train=input_id_mask_train, \n",
    "    targets_train=modified_train_df['meanGrade'],\n",
    "    input_data_valid=input_id_mask_valid,\n",
    "    targets_valid=modified_valid_df['meanGrade'], \n",
    "    batch_size = BATCH_SIZE) \n",
    "print(\"Dataloaders created.\")\n",
    "# calculate loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer_ff_full = torch.optim.Adam(bert_ffnn_full.parameters(), lr=1e-5)\n",
    "\n",
    "bert_model = bert_model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train BERT + FFNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_losses_ff, full_valid_losses_ff = bert_train(\n",
    "     optimizer=optimizer_ff_full, \n",
    "     train_iter=train_loader, \n",
    "     dev_iter=dev_loader, \n",
    "    model=bert_ffnn_full,\n",
    "     loss_fn=loss_fn,\n",
    "    device = device, \n",
    "     number_epoch =10,   \n",
    "     model_name='task1_full_ffnn.pt', \n",
    "    patience=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot Loss***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_vs_epochs(question_train_losses_ff, question_valid_losses_ff, \n",
    "                        full_train_losses_ff,full_valid_losses_ff, title = \"Loss Curves: BERT with FFNN\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. BERT + Convolutional Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***a. CNN with question-sentence preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised.\n",
      "Dataloaders created.\n"
     ]
    }
   ],
   "source": [
    "## CNN question-sentence preprocessing\n",
    "BATCH_SIZE = 64\n",
    "output_channel = [128, 64]\n",
    "\n",
    "bert_cnn = CNN(out_channels=output_channel)\n",
    "print(\"Model initialised.\")\n",
    "\n",
    "bert_cnn.to(device)\n",
    "\n",
    "# Get inputs for BERT and data loaders\n",
    "input_id_mask_train = get_input_bert(df=modified_train_df, tokenizer= tokenizer, col=\"question_edited\")\n",
    "input_id_mask_valid = get_input_bert(df=modified_valid_df, tokenizer= tokenizer, col=\"question_edited\") \n",
    "train_loader, dev_loader = get_dataloaders_no_random_split(\n",
    "    input_data_train=input_id_mask_train, \n",
    "    targets_train=modified_train_df['meanGrade'],\n",
    "    input_data_valid=input_id_mask_valid,\n",
    "    targets_valid=modified_valid_df['meanGrade'], \n",
    "    batch_size = BATCH_SIZE) \n",
    "print(\"Dataloaders created.\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer_cnn = torch.optim.Adam(bert_cnn.parameters(), lr=1e-5)\n",
    "\n",
    "bert_model = bert_model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train BERT + CNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_train_losses, question_valid_losses = bert_train(\n",
    "    optimizer=optimizer_cnn, \n",
    "    train_iter=train_loader, \n",
    "    dev_iter=dev_loader,\n",
    "    loss_fn=loss_fn,  \n",
    "    model=bert_cnn,\n",
    "    device = device,\n",
    "    number_epoch = 10,\n",
    "    patience=5, \n",
    "    model_name='task1_question_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***b. CNN with full-sentence preprocessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised.\n",
      "Dataloaders created.\n"
     ]
    }
   ],
   "source": [
    "## CNN full-sentence preprocessing\n",
    "BATCH_SIZE = 64\n",
    "output_channel = [128, 64]\n",
    "\n",
    "bert_cnn = CNN(out_channels=output_channel)\n",
    "print(\"Model initialised.\")\n",
    "\n",
    "bert_cnn.to(device)\n",
    "\n",
    "# Get inputs for BERT and data loaders\n",
    "input_id_mask_train = get_input_bert(df=modified_train_df, tokenizer= tokenizer, col=\"full_edited\")\n",
    "input_id_mask_valid = get_input_bert(df=modified_valid_df, tokenizer= tokenizer, col=\"full_edited\") \n",
    "train_loader, dev_loader = get_dataloaders_no_random_split(\n",
    "    input_data_train=input_id_mask_train, \n",
    "    targets_train=modified_train_df['meanGrade'],\n",
    "    input_data_valid=input_id_mask_valid,\n",
    "    targets_valid=modified_valid_df['meanGrade'], \n",
    "    batch_size = BATCH_SIZE) \n",
    "print(\"Dataloaders created.\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer_cnn = torch.optim.Adam(bert_cnn.parameters(), lr=1e-5)\n",
    "\n",
    "bert_model = bert_model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train BERT + CNN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_losses, full_valid_losses = bert_train(\n",
    "    optimizer=optimizer_cnn, \n",
    "    train_iter=train_loader, \n",
    "    dev_iter=dev_loader,\n",
    "    loss_fn=loss_fn,  \n",
    "    device=device,\n",
    "    model=bert_cnn, \n",
    "    number_epoch = 10,\n",
    "    patience=5, \n",
    "    model_name='task1_full_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot Loss***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_vs_epochs(question_train_losses, question_valid_losses, \n",
    "                        full_train_losses,full_valid_losses, title = \"Loss Curves: BERT with CNN\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Testing with BERT+CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNN:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\". \n\tUnexpected key(s) in state_dict: \"ffnn.0.0.weight\", \"ffnn.0.0.bias\", \"ffnn.1.0.weight\", \"ffnn.1.0.bias\", \"ffnn.2.0.weight\", \"ffnn.2.0.bias\". \n\tsize mismatch for output.0.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([1, 64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-66431a11dbb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbert_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbert_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"task1_question_ffnn.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#cnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model (Task 1) loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mbert_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNN:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\". \n\tUnexpected key(s) in state_dict: \"ffnn.0.0.weight\", \"ffnn.0.0.bias\", \"ffnn.1.0.weight\", \"ffnn.1.0.bias\", \"ffnn.2.0.weight\", \"ffnn.2.0.bias\". \n\tsize mismatch for output.0.weight: copying a param with shape torch.Size([1, 32]) from checkpoint, the shape in current model is torch.Size([1, 64])."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "test_id_mask = get_input_bert(df=modified_test_df,tokenizer=tokenizer, col=\"full_edited\") \n",
    "test_loader = torch.utils.data.DataLoader(test_id_mask, \n",
    "                                          shuffle=False, \n",
    "                                          batch_size=64)\n",
    "\n",
    "bert_cnn = CNN(out_channels=[128, 64])\n",
    "bert_cnn.load_state_dict(torch.load(\"task1_question_cnn.pt\")) \n",
    "print(\"Model (Task 1) loaded.\")\n",
    "bert_cnn.to(device=device)\n",
    "bert_model = bert_model.to(device=device)\n",
    "# evaluate model\n",
    "bert_cnn.eval()\n",
    "cnn_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(\"Start testing ...\")\n",
    "  for (test_id, test_mask) in test_loader:\n",
    "    test_id = test_id.to(device=device, dtype=torch.long)\n",
    "    test_mask = test_mask.to(device=device, dtype=torch.long)\n",
    "    pred = bert_cnn(test_id, test_mask)\n",
    "    cnn_pred.append(pred.cpu().numpy())\n",
    "\n",
    "cnn_pred = np.concatenate(cnn_pred)\n",
    "test_df = modified_test_df[['id']]\n",
    "test_df.loc[:, 'pred'] = cnn_pred.flatten()\n",
    "\n",
    "# save to csv\n",
    "task1_truth = pd.read_csv(\"data/task-1/truth.csv\")\n",
    "assert(sorted(task1_truth.id) == sorted(test_df.id)),\"ID mismatch between ground truth and prediction!\"\n",
    "data = pd.merge(task1_truth, test_df)\n",
    "rmse = np.sqrt(np.mean((data['meanGrade'] - data['pred'])**2))\n",
    "print(\"RMSE = %.3f\" % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
