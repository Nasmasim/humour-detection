{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this Notebook, we present the baseline approach using GloVe word embeddings combined with a biLSTM to predict the funniness score of a news headline from the humicroedit dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural language Processing, humour detection is a challenging task. The [SemEval-2020 Task 7](https://arxiv.org/pdf/2008.00304.pdf)  aims to detect humour in English news headlines from micro-edits. The humicroedit dataset (Hossain et al., 2020) contains 9653 training, 2420 development and 3025 testing examples. In task 1 the goal is to predict the the funniness score of an edited headline in the ranges of 0 to 3, where 0 means not funny and 3 means very funny. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Import and Downloads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\n",
      "--2021-05-22 18:30:14--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-05-22 18:30:14--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-05-22 18:30:15--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822,24M  3,58MB/s    in 5m 57s  \n",
      "\n",
      "2021-05-22 18:36:14 (2,30 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "#@title Download and Install\n",
    "# Check GPU\n",
    "!nvidia-smi\n",
    "# Baseline data collection\n",
    "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip -O glove.6B.zip\n",
    "!unzip -n glove.6B.zip\n",
    "# installing transformers\n",
    "!pip -q install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "# Library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from preprocessing.preprocessor import (create_edited_sentences, dataset_question_full_processing,\n",
    "                                        create_custom_vocab, get_stop_words)\n",
    "from dataloader.data_loaders import (Task1Dataset, collate_fn_padd, get_input_bert,\n",
    "                                      get_dataloaders, get_dataloaders_no_random_split)\n",
    "from models.biLSTM import BiLSTM\n",
    "from trainer.biLSTM_trainer import biLSTM_train, biLSTM_eval\n",
    "from utils.plot import (plot_sentence_length_stopwords, plot_mean_grade_distribution,\n",
    "                        plot_number_characters, plot_number_words, plot_top_ngrams, \n",
    "                        plot_loss_vs_epochs)\n",
    "from utils.ngrams import get_top_ngram\n",
    "from utils.vocab import create_vocab, get_word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Settings and Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Torch Settings\n",
    "# Setting random seed and device\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Number of epochs\n",
    "epochs_bl = 10 \n",
    "epochs = 100\n",
    "\n",
    "# Proportion of training data for train compared to dev\n",
    "train_proportion = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('data/task-1/train.csv')\n",
    "dev_df = pd.read_csv('data/task-1/dev.csv')\n",
    "test_df = pd.read_csv('data/task-1/test.csv')\n",
    "\n",
    "# Convert them to full edited sentences\n",
    "modified_train_df = create_edited_sentences(train_df)\n",
    "modified_valid_df = create_edited_sentences(dev_df)\n",
    "modified_test_df = create_edited_sentences(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Stop Words***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nasmadasser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk stopwords english list\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = list(stopwords.words('english'))\n",
    "\n",
    "# import custom stopword list\n",
    "simple_stopwords, custom_stopwords = get_stop_words()\n",
    "\n",
    "all_stopwords = list(set(custom_stopwords + nltk_stopwords))\n",
    "\n",
    "stopwords_lists = [[],simple_stopwords, custom_stopwords,nltk_stopwords,\\\n",
    "                   all_stopwords]\n",
    "edited_modes = ['question_edited','full_edited']\n",
    "\n",
    "# OK Applying both question and full version to any dataframe and dropping useless values\n",
    "modified_train_df = dataset_question_full_processing(modified_train_df)\n",
    "modified_valid_df = dataset_question_full_processing(modified_valid_df)\n",
    "modified_test_df = dataset_question_full_processing(modified_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Word2idx***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab created.\n"
     ]
    }
   ],
   "source": [
    "#@title Word2idx call on the original sentences\n",
    "## Approach 1 code, using functions defined above:\n",
    "\n",
    "# We set our training data and test data\n",
    "training_data = train_df['original']\n",
    "test_data = dev_df['original']\n",
    "\n",
    "# Creating word vectors\n",
    "training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
    "test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
    "# Creating joint vocab from test and train:\n",
    "joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data]))\n",
    "print(\"Vocab created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path ='./embeddings/glove.6B.100d.txt'\n",
    "wvecs, word2idx, idx2word= get_word2idx(file_path, joint_vocab)\n",
    "\n",
    "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\n",
    "\n",
    "# To avoid any sentences being empty (if no words match to our word embeddings)\n",
    "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Train GloVE + biLSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised.\n",
      "Dataloaders created.\n"
     ]
    }
   ],
   "source": [
    "#@title Train BiLSTM on original sentences\n",
    "INPUT_DIM = len(word2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = BiLSTM(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\n",
    "print(\"Model initialised.\")\n",
    "\n",
    "model.to(device)\n",
    "# We provide the model with our embeddings\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "\n",
    "feature = vectorized_seqs\n",
    "\n",
    "# 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
    "train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\n",
    "\n",
    "train_examples = round(len(train_and_dev)*train_proportion)\n",
    "dev_examples = len(train_and_dev) - train_examples\n",
    "\n",
    "train_dataset, dev_dataset = random_split(train_and_dev,\n",
    "                                           (train_examples,\n",
    "                                            dev_examples))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "\n",
    "print(\"Dataloaders created.\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
      "| Epoch: 02 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
      "| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
      "| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
      "| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
      "| Epoch: 06 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
      "| Epoch: 07 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n",
      "| Epoch: 08 | Train Loss: 0.25 | Train MSE: 0.25 | Train RMSE: 0.50 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.62 |\n",
      "| Epoch: 09 | Train Loss: 0.24 | Train MSE: 0.24 | Train RMSE: 0.49 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.62 |\n",
      "| Epoch: 10 | Train Loss: 0.22 | Train MSE: 0.22 | Train RMSE: 0.47 |         Val. Loss: 0.39 | Val. MSE: 0.39 |  Val. RMSE: 0.62 |\n"
     ]
    }
   ],
   "source": [
    "biLSTM_train(train_loader, dev_loader, model, epochs_bl, device, optimizer, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
